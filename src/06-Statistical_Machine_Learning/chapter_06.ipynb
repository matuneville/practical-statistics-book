{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _K_-Nearest Neighbors\n",
    "\n",
    "- K-Nearest Neighbors (KNN) classifies a record by assigning it to the class that similar records belong to.\n",
    "- Similarity (distance) is determined by Euclidean distance or other related metrics.\n",
    "- The number of nearest neighbors to compare a record to, K, is determined by how well the algorithm performs on training data, using different values for K.\n",
    "- Typically, the predictor variables are standardized so that variables of large scale do not dominate the distance metric.\n",
    "- KNN is often used as a first stage in predictive modeling, and the predicted value is added back into the data as a predictor for second-stage (non-KNN) modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [**Heart Disease classification**](https://github.com/matuneville/ml-projects/blob/main/projects/heart-attack-prediction-%26-analysis/heart-attack.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree models: Bagging and Random Forest\n",
    "\n",
    "- Ensemble models improve model accuracy by combining the results from many models.\n",
    "- Bagging is a particular type of ensemble model based on fitting many models to bootstrapped samples of the data and averaging the models.\n",
    "- Random forest is a special type of bagging applied to decision trees. In addition to resampling the data, the random forest algorithm samples the predictor variables when splitting the trees.\n",
    "- A useful output from the random forest is a measure of variable importance that ranks the predictors in terms of their contribution to model accuracy.\n",
    "- The random forest has a set of hyperparameters that should be tuned using cross-validation to avoid overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [**Passengers per flight prediction**](https://github.com/matuneville/ml-projects/blob/main/projects/airports/air.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting\n",
    "\n",
    "- Boosting is a class of ensemble models based on fitting a sequence of models, with more weight given to records with large errors in successive rounds.\n",
    "- Stochastic gradient boosting is the most general type of boosting and offers the best performance. The most common form of stochastic gradient boosting uses tree models.\n",
    "- XGBoost is a popular and computationally efficient software package for stochastic gradient boosting; it is available in all common languages used in data science.\n",
    "- Boosting is prone to overfitting the data, and the hyperparameters need to be tuned to avoid this.\n",
    "- Regularization is one way to avoid overfitting by including a penalty term on the number of parameters (e.g., tree size) in a model.\n",
    "- Cross-validation is especially important for boosting due to the large number of hyperparameters that need to be set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **TODO project**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
